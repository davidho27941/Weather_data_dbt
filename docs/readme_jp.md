# weather_data_dbt


## イントロダクション
これは、エンドツーエンドの自動データパイプラインを構築するサイドプロジェクトです。データソースとして台湾政府が提供する天気データを使用しています。この自動パイプラインは、広く知られている自動化ツールであるApache Airflowによって駆動されます。Airflowタスクでダウンロードされたデータは、AWS S3ストレージにアップロードされます。AWS S3は、Snowflakeデータウェアハウスの外部テーブルとして設定されています。

![Overview](../images/jp/project_overview_jp.jpg)


## 必要要件

* Airflow >= 2.9.3
* dbt-core == 1.8.5
* dbt-snowflake == 1.8.3
* astronomer-cosmos == 1.5.1
* boto3 == 1.34.90

### オプション
* Docker >= 27.0.3
* k3s == v1.30.4+k3s1


## 抽出・ロードプロセス

このプロジェクトの自動データパイプラインは、セルフホストされたApache Airflowに基づいており、AWS S3をリモートストレージとして使用しています。DAGは異なる頻度でスケジュールされ、台湾政府が提供するオープンデータプラットフォームから目的のデータを取得します。取得されたデータはAWS S3にアップロードされます。

![Extract-Load](../images/jp/extract_load_jp.jpg)


## 変換プロセス
データ変換パイプラインでは、S3ストレージをSnowflakeの外部テーブルとして使用し、dbt（データビルドツール）を利用して外部テーブルからデータを変換します。変換プロセスは、ステージング、中間、およびアーティファクトの3つのステップで構成され、それぞれ異なる目的に対応しています。

![Transformation](../images/jp/transformation_jp.jpg)


> 詳細な変換および処理情報については、自動生成されたdbtドキュメントを参照してください: [Web Page](https://davidho27941.github.io/weather_data_dbt/#!/overview)

## 将来の計画
現在の設計では、HTTP接続ディレクトリによるリアルタイムデータの取得を行っていますが、これはシンプルである一方で、オペレーターが予期しないエラーに直面した際にデータ取得に失敗する可能性があります。Kafkaデータストリーミングの実装は、フォールトトレランスのための潜在的な解決策です。